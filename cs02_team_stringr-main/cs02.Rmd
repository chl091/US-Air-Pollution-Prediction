---
title: "CS02 - Predicting Annual Air Pollution"
author: "Jonathan Ito, Qinwen Deng, Chunjiang Liu, Jessica De La Torre, William Zhao"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

## Introduction

```{r setup, include=FALSE}
# control global Rmd chunk settings
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Throughout the U.S., air pollution has been an increasingly alarming issue, as different pollutants become more prominent . Currently, air pollutants can be divided into four different categories and three different particulates. The type categories include: gaseous, particulate, dust, and biological. These particulate categories are the following: large coarse particulates (diameter greater than 10 µm), coarse particulate matter (diameter between 2.5-10 µm), and fine particulate matter (diameter less than 2.5 µm) (COGS 137 - Lecture: 09-cs02-intro; U.S. Environmental Protection Agency). From these categories, fine particulate matter poses the greatest health threat. Fine particles are the most harmful from the three due to the fact that they are more easily absorbed into the body, to the point where they can go so far as to be penetrated into the lung alveoli (COGS 137 - Lecture: 09-cs02-intro; Manisalidis et al.). Regardless of size, air pollution has the potential to damage one's health significantly. Specifically, exposure to air pollution can lead to a variety of conditions including asthma, inflammation in type 1 diabetes, viral infections, and in extreme cases even death (COGS 137 - Lecture: 09-cs02-intro; Puett et al.).  Thus, it is important to gather information about these types of air  pollution and how they affect the people that are faced with air pollution in their communities. 

In past studies, the effects of pollution on communities have previously been studied, yet there are still some issues with the methods and limitations. A notable issue is that the monitors that are used to track air pollution are placed in limited locations across the U.S., especially within cities. This is an issue because significant changes in air pollution can be noticed in micro-environments within the same city (COGS 137 - Lecture: 09-cs02-intro; Yanosky et al.). However, in this study, the intention is to understand with what accuracy can machine learning help us predict the U.S. annual average air pollution concentrations. It is understood that machine learning can help overcome the sparsity of monitors across the U.S. In doing this, we can account for the lack of data in micro-environments, and collect a more comprehensive understanding of air pollution rates across the U.S. To address these limitations, this study leverages machine learning to predict U.S. annual average air pollution concentrations and explores how accurately these predictions can be made. Additionally, it examines how prediction accuracy differs between Random Forest and Linear Regression models in counties with high and low population densities.

## Questions

### Main Question

With what accuracy can we predict US annual average air pollution concentrations?

### Extension Question

How does the prediction accuracy differ in counties with high and low population densities when using a Random Forest prediction model versus a Linear prediction model?

### Load packages

```{r load-packages, message=FALSE}
library(tidymodels)
library(tidyverse)
library(skimr)
library(GGally)
```


## The Data

The dataset is retrieved from Open Case Studies: Predicting Annual Air Pollution (Wright et al., 2020). The dataset contains air pollution measurements from the US Environmental Protection Agency (EPA) and several other sources, including NASA, the US Census, and the National Center for Health Statistics (NCHS). It includes data from 876 monitors across the contiguous United States, each providing annual averages of fine particulate matter (PM2.5) concentration, measured in micrograms per cubic meter (µg/m³). The outcome variable is the PM2.5 concentration, captured by gravimetric monitors, which are equipped with filtration systems that specifically measure fine particulate matter.

In addition to the monitor data, the dataset includes a variety of predictor variables, including geographical data (latitude, longitude, and FIPS code), demographic data (population density, educational attainment, and poverty levels), environmental data (impervious surface measures and proximity to primary and secondary roads), satellite-based data (aerosol optical depth (AOD) measurements from NASA, a proxy for particulate pollution), and other variables (proximity to major emission sources and county-level urban-rural classification).

### Data Import

```{r}
pm <- read_csv("data/pm25_data.csv")
```

### Data Wrangling

Some of the variables don't have very informative names, so as part of the data wrangling we will be renaming those variables.

```{r}
pm <- pm |>
  rename(impervious_surface_500 = imp_a500,
         impervious_surface_1000 = imp_a1000,
         impervious_surface_5000 = imp_a5000,
         impervious_surface_10000 = imp_a10000,
         impervious_surface_15000 = imp_a15000,
         
         log_pm25_emmissions_10000 = log_nei_2008_pm25_sum_10000,
         log_pm25_emmissions_15000 = log_nei_2008_pm25_sum_15000,
         log_pm25_emmissions_25000 = log_nei_2008_pm25_sum_25000,
         log_pm10_emmissions_10000 = log_nei_2008_pm10_sum_10000,
         log_pm10_emmissions_15000 = log_nei_2008_pm10_sum_15000,
         log_pm10_emmissions_25000 = log_nei_2008_pm10_sum_25000,
         
         urban_level_2013 = urc2013,
         urban_level_2006 = urc2006)
```

We also want to make sure that the data is in the right format. We need some of our data to be factors, so let's do that.

```{r}
pm <- pm |>
  mutate(across(c(id, fips, zcta), as.factor))
```

Lastly, we're gonna want a long version of our dataset in order to generate some plots, so we'll make that now.

```{r}
pm_long <- pm |>
  pivot_longer(cols = starts_with("impervious"),
               names_to = "impervious_surfaces",
               values_to = "impervious_surfaces_values") |>
  pivot_longer(cols = starts_with("log_pri_"),
               names_to = "pri_roads",
               values_to = "pri_roads_values") |>
  pivot_longer(cols = starts_with("log_prisec"),
               names_to = "prisec_roads",
               values_to = "prisec_roads_values") |>
  pivot_longer(cols = starts_with("log_pm"),
               names_to = "pm_emissions",
               values_to = "pm_emissions_values")

pm_long
```


## EDA

The first thing we would like to do is just get an overall look at the data, and to do that we will use skimr. This function gives us some quick, but important, statistics about our data that can be very useful for the rest of the EDA and Analysis that we do.

```{r}
skim(pm)
```

Here we can see that there are 876 rows/observations and 50 columns/variables. We can also see that the value of n_missing in every variable is 0, indicating that there are no null values in the data. The skim() function also shows the mean, standard deviation, and quantile values for all of the numerical variables.

We have plenty of numerical variables, so let's see what some of them look like. There are a lot of variables for impervious surfaces, primary and secondary road lengths, and PM 10/2.5 emissions. We'll start with the impervious surfaces measurements first.

```{r}
pm_long |>
  ggplot(aes(y = impervious_surfaces_values, fill = impervious_surfaces)) + 
  geom_boxplot() + 
  facet_grid(. ~ impervious_surfaces, ) + 
  labs(
    title = "Impervious Surface Values at Different Radii",
    subtitle = "The radii of 500m and 1000m have the highest values",
    x = "Impervious Surface Measurements"
  ) + 
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    strip.text.x = element_text(angle = 0, size = 7),
    axis.text.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )
```

It looks like the highest values are in the 500 meter radius while the lowest values are in the 15000 meter radius. This is possibly because in a larger radius there is more empty land that is measured than in the smaller radius.

For the road lengths, we'll only be looking at the variables for primary roads only.

```{r}
pm_long |>
  ggplot(aes(y = pri_roads_values, fill = pri_roads)) + 
  geom_boxplot() + 
  facet_grid(. ~ pri_roads) + 
  labs(
    title = "Count of Primary Road Length at Different Radii From Monitor",
    subtitle = "Count of road length gets progressively higher with a bigger radius",
    x = "Primary Road Length"
  ) + 
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )
```

It looks like the count of road length gets higher as the radius around the monitor increases. This makes sense as the roads included in the 500 meter radius are also included in the 10000 meter radius.

The last set of variables we want to look at are the emissions variables.

```{r}
pm_long |>
  filter(str_starts(pm_emissions, "log_pm25")) |>
  ggplot(aes(y = pm_emissions_values, fill = pm_emissions)) + 
  geom_boxplot() +
  facet_grid(. ~ pm_emissions) +
  labs(
    title = "PM 2.5 Emissions From Different Radii",
    subtitle = "PM 2.5 emissions grow as the radius gets larger",
    x = "PM 2.5 Emissions"
  ) + 
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.title.y = element_blank(),
    legend.position = "none"
  )
```

Just like with the road length, the PM 2.5 emissions grow larger as the radius around the monitor gets larger. Again, this makes sense for the same reasons as with the road length. This might suggest that these types of variables are closely correlted within their groups.

Another thing that we saw with the skim output is that there are only 49 states. Why is that? Let's find out

```{r}
pm |>
  distinct(state)
```

Looking at the output, and knowing a little bit of American geography, we can see that Alaska and Hawaii aren't included, but Washington DC is which explains why there are 49 "states"

Still working with the states, one thing that can affect the outcome of the model is how many monitors there are per state

```{r}
pm |> group_by(state) |>
  summarize(num_monitors = n()) |>
  arrange(desc(num_monitors))
```

California clearly has the most amount of monitors, with Ohio in second with nearly half the number. On the other end, Maine only has 1 monitor.

Why does California have so many monitors? Maybe it's because they have much higher pollution than other states? Let's find out.

```{r}
pm <- pm |>
  mutate(cali = case_when(
    state == "California" ~ "California",
    state != "California" ~ "Not California"
  ))

pm |> 
  ggplot(aes(x = cali, y = value, fill = cali)) + 
  geom_boxplot() +
  labs(
    title = "Pollution Levels in California Vs. the Rest of the US",
    subtitle = "California has more pollution than the average of the rest of the US",
    x = "California Vs. Not California",
    y = "Pollution Levels"
  ) + 
  theme_minimal() + 
  scale_fill_manual(values = c("California" = "skyblue", "Not California" = "coral")) +
  theme(
    legend.position = "none"
  )
```

It looks like California, on average, does have more pollution than everywhere else in the US. However we can see that it also has more variability as the quantiles are more spred out.

California has the most monitors and also has higher than average pollution, so this begs the question if the number of monitors is correlated with the levels of pollution for other states as well.

```{r}
pm_state <- pm |>
  group_by(state) |>
  summarize(num_monitors = n(), avg_pol = mean(value))

pm_state |>
  ggplot(aes(x = num_monitors, y = avg_pol)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  labs(
    title = "Average Pollution Levels Rise with Number of Monitors",
    subtitle = "The average pollution and number of monitors are at the state level",
    x = "Numer of Monitors",
    y = "Average Pollution"
  ) + 
  theme_minimal() + 
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_blank()
  )
```

In this plot we can see that there is a definite positive relationship between the number of monitors and the average pollution. Without further analysis it is impossible to decide whether this is causal in any way, but it's interesting to keep in mind nonetheless. Maybe states with more pollution receive more monitors?

We know that the number of monitors is correlated with the pollution level, but maybe there are some confounding variables that are pushing this correlation? One variable in the dataset that could help explain this is the population density of each county. Perhaps a higher population density results in more pollution?

```{r}
pm |> 
  ggplot(aes(x = popdens_county, y = value)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs(
    title = "Pollution Levels Slightly Correlated with County Population Density",
    subtitle = "Population density is generally low, with a couple of outliers",
    x = "County Population Density",
    y = "Pollution Levels"
  ) + 
  theme_minimal() + 
  theme(
    
  )
```

It seems like there is a slight positive correlation, however most of the data points have very low population density so there's no clear relationship.

The number of monitors in each state and the population density are only two variables that may have predictive power for pollution levels. Our dataset has ~50 variables that we can use, but it's best if we can narrow that number down to only use the variables with the most predictive power. One way to do this is by looking at the correlation between the variables in our dataset. Variables that are highly correlated with each other may not add much predictive power, and may actually make the model worse.

```{r}
PM_cor <- cor(pm |> dplyr::select_if(is.numeric))
corrplot::corrplot(PM_cor, tl.cex = 0.5)
```

From this plot we can see that all of the pollution measurements are very correlated with each other, especially the measurements taken in the same radius. We can also see that the measurements for development are all highly correlated and the measurements for road density are all correlated as well. Interestingly, all of the education level variables have near zero correlation with the other variables. Lastly, the value variable (our outcome variable) has no or near-zero correlation with the other variables as well. This might open the door for interaction effects.

We can see from the table that the development variables, road density variables, and pollution measurement variables are all related within each group, but just how correlated are they?

First we'll look at the development variables (imp)

```{r}
select(pm, contains("imp")) |>
  GGally::ggpairs() + 
  labs(
    title = "Correlation and Pairwise Plots of Impervious Surfaces",
    subtitle = "Strong correlation between all radiuses",
    x = "Impervious Surface Measures (ranging from 500m to 15000m)"
  )
```

All of the variables are very related to each other, but the measurements taken at the same radius are nearly perfectly correlated.

Now let's look at the road density variables

```{r}
select(pm, contains("pri")) |>
  GGally::ggcorr(hjust = .85, size = 3,
       layout.exp=2, label = TRUE) + 
  labs(
    title = "Correlation Matrix for Primary and Secondary Road Variables",
    subtitle = "Negative correlation between the distance from a monitor to a road and the length of a road",
    x = "Road Variable Name"
  )
```

From this table we can see that the distance from a monitor to a primary or secondary road is negatively correlated with the road length variables, which makes sense. If the distance is higher, then that means there are less roads in the area.

Finally, let's look at the emmissions variables.

```{r}
select(pm, contains("pm")) |>
  GGally::ggpairs() + 
  labs(
    title = "Correlation and Pairwise Plots of PM 10 and PM 2.5 Emissions",
    subtitle = "Very strong correlation between PM 10 and PM 2.5 when taken at the same radius",
    x = "PM 2.5 Variables (First 3) and PM 10 Variables (Last 3)"
  )
```

Again we see the same pattern as with the development variables. They're all very positively correlated with each other, but the variables that share the same radius of measurement have the highest correlation (near perfect)

So we know that all of those variables are correlated within each other, but how about between each other?

```{r}
pm |>
select(log_pm25_emmissions_10000, popdens_county, 
       log_pri_length_10000, impervious_surface_10000, county_pop) |>
  GGally::ggpairs() + 
  labs(
    title = "Correlation and Pairwise Plots for Variables of Interest",
    subtitle = "Emissions, population density, primary road length, impervious surface measurements, population"
  )
```

We can see that the groups generally aren't very correlated with one another. The development marker and the road length marker at a radius of 10,000 meters are the most correlated, with a correlation coefficient of .649.

## Analysis


```{r}
# Converting to factors
pm <- pm |>
  mutate(across(c(id, fips, zcta), as.factor)) 
```

### Data Splitting
```{r}
pm <- pm |>
  mutate(city = case_when(city == "Not in a city" ~ "Not in a city",
                          city != "Not in a city" ~ "In a city"))

set.seed(1234) # same seed as before
pm_split <- rsample::initial_split(data = pm, prop = 2/3)
pm_split

train_pm <- rsample::training(pm_split)
test_pm <- rsample::testing(pm_split)
```

### Initial Model
```{r}
RF_rec <- recipe(train_pm) |>
    update_role(everything(), new_role = "predictor")|>
    update_role(value, new_role = "outcome")|>
    update_role(id, new_role = "id variable") |>
    update_role("fips", new_role = "county id") |>
    step_novel("state") |>
    step_string2factor("state", "county", "city") |>
    step_rm("county") |>
    step_rm("zcta") |>
    step_corr(all_numeric())|>
    step_nzv(all_numeric())
```

```{r}
RF_PM_model <- parsnip::rand_forest(mtry = 10, min_n = 3) |> 
  set_engine("randomForest") |>
  set_mode("regression")

RF_PM_model
```

```{r}
RF_wflow <- workflows::workflow() |>
  workflows::add_recipe(RF_rec) |>
  workflows::add_model(RF_PM_model)

RF_wflow
```

```{r}
RF_wflow_fit <- parsnip::fit(RF_wflow, data = train_pm)

RF_wflow_fit
```

The RMSE of our model is 2.7 while the R-squared is 58.26%. So while our RMSE is decently low, we aren't explaining a lot of the variance in the data with our model. Our model has a lot of variables, so let's see which ones are contributing the most.

```{r}
RF_wflow_fit |> 
  extract_fit_parsnip() |> 
  vip::vip(num_features = 10) + 
  labs(
    title = "Ranked Predictor Variable Importance",
    subtitle = "State is by far the most 'important' predictor",
    y = "Importance Level",
    x = "Variable Name"
  ) + 
  theme_minimal() + 
  theme(
    axis.text.x = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
```

Here we see that the state is the most important variable for determining pollution levels. As we saw in an earlier plot, California had on average higher pollution than the other states, so maybe a state being California or not is a large predictor.

### Implementing Cross-Validation

While our model is usable, it can be improved on pretty easily. One way to do this is by using cross-validation to ensure less overfitting by the model on the training data.

```{r}
set.seed(1234)
vfold_pm <- rsample::vfold_cv(data = train_pm, v = 4)
pull(vfold_pm, splits)
```

```{r}
set.seed(456)
resample_RF_fit <- tune::fit_resamples(RF_wflow, vfold_pm)
collect_metrics(resample_RF_fit)
```

Looking at the RMSE now, we can see that it's dropped by a lot (relative to the size of it before cross-validation). On the other hand, the R-squared has only increased minimally, by roughly .06 percentage points.

Using cross-validation is a step in the right direction, but we can take this even further by tuning the hyperparameters as well.

### Tuning Our Hyperparameters

```{r}
tune_RF_model <- rand_forest(mtry = tune(), min_n = tune()) |>
  set_engine("randomForest") |>
  set_mode("regression")

tune_RF_model
```

Now let's create the worflow using the tuned model

```{r}
RF_tune_wflow <- workflows::workflow() |>
  workflows::add_recipe(RF_rec) |>
  workflows::add_model(tune_RF_model)

RF_tune_wflow
```

We'll use all of the cores we have available in order to tune the model the fastest

```{r}
n_cores <- parallel::detectCores()
n_cores
```

```{r}
doParallel::registerDoParallel(cores = n_cores)

set.seed(123)
tune_RF_results <- tune_grid(object = RF_tune_wflow, resamples = vfold_pm, grid = 20)
tune_RF_results
```

```{r}
tune_RF_results |>
  collect_metrics()

show_best(tune_RF_results, metric = "rmse", n = 1)
```

```{r}
tuned_RF_values <- select_best(tune_RF_results, metric = "rmse")
tuned_RF_values
```

The best model of all the ones tried had a mean RMSE of 1.66, which is lower than what it was previously. It also had an mtry of 31 and a min_n of 4, meaning that the model selects 31 random variables for each tree and the terminal leaves have to have at least 4 datapoints. A min_n of 4 is a low number, so let's see how our model generalized onto the testing data.

```{r}
# specify best combination from tune in workflow
RF_tuned_wflow <-RF_tune_wflow |>
  tune::finalize_workflow(tuned_RF_values)

# fit model with those parameters on train AND test
overallfit <- RF_wflow |>
  tune::last_fit(pm_split)

collect_metrics(overallfit)
```

Our final model has a RMSE of 1.72 and a R-squared of 60.3%. It seems like our model did not overfit the training data at all. We were also able to squeeze an extra 2 percentage points for the R-squared by tuning the hyperparameters as well.

Now that we've predicted our test data, let's gather all of the predictions so we can use them later.

```{r}
test_predictions <- collect_predictions(overallfit)
```

### Plotting the model prediction result

We have our predictions and a metric for determining it's accuracy, but it would be easier if we could visualize how accurate our model is. In order to do this we will need to plot the geographical data of our monitors as well as our predictions.

```{r}
library(sf)
library(maps)
library(rnaturalearth)
```

Here is a world map that we will be building off of.

```{r}
world <- ne_countries(scale = "medium", returnclass = "sf")

ggplot(data = world) +
    geom_sf() 
```

Our data is only in the US, so let's narrow down our map to just the continental US.

```{r}
ggplot(data = world) +
    geom_sf() +
    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), 
             expand = FALSE)
```

We've narrowed down the map to just the continental US, but we still want to see where our monitors are on the map.

```{r}
ggplot(data = world) +
    geom_sf() +
    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), 
             expand = FALSE)+
    geom_point(data = pm, aes(x = lon, y = lat), size = 2, 
               shape = 23, fill = "darkred")
```

Our data is also separated by county, so let's add the county lines as well.

```{r}
counties <- sf::st_as_sf(maps::map("county", plot = FALSE,
                                   fill = TRUE))

monitors <- ggplot(data = world) +
    geom_sf(data = counties, fill = NA, color = gray(.5))+
      coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), 
             expand = FALSE) +
    geom_point(data = pm, aes(x = lon, y = lat), size = 2, 
               shape = 23, fill = "darkred") +
    ggtitle("Monitor Locations") +
    theme(axis.title.x=element_blank(),
          axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank())

monitors
```

Before we move on, we need to wrangle the data a little bit more in order to make the plots work.

```{r}
counties <- counties |> 
  tidyr::separate(ID, into = c("state", "county"), sep = ",") |> 
  dplyr::mutate(county = stringr::str_to_title(county))

map_data <- dplyr::inner_join(counties, pm, by = "county")
```

Now we can plot the actual pollution levels vs. our predicted pollution levels.

First, the ground truth levels.

```{r}
truth <- ggplot(data = world) +
  coord_sf(xlim = c(-125,-66),
           ylim = c(24.5, 50),
           expand = FALSE) +
  geom_sf(data = map_data, aes(fill = value)) +
  scale_fill_gradientn(colours = topo.colors(7),
                       na.value = "transparent",
                       breaks = c(0, 10, 20),
                       labels = c(0, 10, 20),
                       limits = c(0, 23.5),
                       name = "PM ug/m3") +
  ggtitle("True PM 2.5 levels") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

truth
```

And now the predicted data. First, we need to get the data.

```{r}
# fit data
RF_final_train_fit <- parsnip::fit(RF_tuned_wflow, data = train_pm)
RF_final_test_fit <- parsnip::fit(RF_tuned_wflow, data = test_pm)

# get predictions on training data
values_pred_train <- predict(RF_final_train_fit, train_pm) |> 
  bind_cols(train_pm |> select(value, fips, county, id)) 

# get predictions on testing data
values_pred_test <- predict(RF_final_test_fit, test_pm) |> 
  bind_cols(test_pm |> select(value, fips, county, id)) 
values_pred_test

# combine
all_pred <- bind_rows(values_pred_test, values_pred_train)
```

And now we can plot it.

```{r}
map_data <- inner_join(counties, all_pred, by = "county")

pred <- ggplot(data = world) +
  coord_sf(xlim = c(-125,-66),
           ylim = c(24.5, 50),
           expand = FALSE) +
  geom_sf(data = map_data, aes(fill = .pred)) +
  scale_fill_gradientn(colours = topo.colors(7),
                       na.value = "transparent",
                       breaks = c(0, 10, 20),
                       labels = c(0, 10, 20),
                       limits = c(0, 23.5),
                       name = "PM ug/m3") +
  ggtitle("Predicted PM 2.5 levels") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

pred
```

Now that we have both plots, let's look at them against each other.

```{r}
library(patchwork)

final_plot <- (truth/pred) + 
  plot_annotation(title = "Machine Learning Methods Allow for Prediction of Air Pollution", subtitle = "A random forest model predicts true monitored levels of fine particulate matter (PM 2.5) air pollution based on\ndata about population density and other predictors reasonably well, thus suggesting that we can use similar methods to predict levels\nof pollution in places with poor monitoring",
                  theme = theme(plot.title = element_text(size =12, face = "bold"), 
                                plot.subtitle = element_text(size = 8)))

final_plot
```

It looks like our Random Forest model is predicting the pollution for each available county across the US very well. There is some variation, but that is expected. Overall it seems like our model is more than accurate enough. However, are there ways we can improve this?

## Extension Question

For our extension question, we want to know if there is a difference with how our model performs with high population density and low population density counties, as well as if a linear model might be better for that task. We'll start by separating our dataset into a high population density dataset and a low population density dataset.

```{r}
median_popdens = median(pm$popdens_county, na.rm = TRUE)

# Split into high and low population density
high_density <- pm |> filter(popdens_county > median_popdens)
low_density <- pm |> filter(popdens_county <= median_popdens)

# Check the number of rows in each group
cat("High Density Counties:", nrow(high_density), "\n")
cat("Low Density Counties:", nrow(low_density), "\n")
```

There are a very even number of observations in each dataset, so that won't be a problem for analysis.

```{r}
# Fit a linear regression model for high population density counties
lm_high <- lm(CMAQ ~ popdens_county + impervious_surface_1000 + log_pm25_emmissions_10000, data = high_density)

# Fit a linear regression model for low population density counties
lm_low <- lm(CMAQ ~ popdens_county + impervious_surface_1000 + log_pm25_emmissions_10000, data = low_density)

# Summarize the models
summary(lm_high)
summary(lm_low)
```

The RMSE for the high population density data is 2.32 and the RMSE for the low population density data is 2.67. This is a relatively large difference between them, which implies that one model might not be sufficient.

```{r}
# Extract predictions for high-density and low-density groups
rf_pred_high <- predict(overallfit$.workflow[[1]], new_data = high_density) |>
  bind_cols(high_density$value) |>
  rename(value = "...2")

rf_pred_low <- predict(overallfit$.workflow[[1]], new_data = low_density) |>
  bind_cols(low_density$value) |>
  rename(value = "...2")
```

```{r}
rf_pred_high |>
  rmse(truth = value, estimate = .pred)
```

```{r}
rf_pred_low |>
  rmse(truth = value, estimate = .pred)
```

```{r}
# Visualization: RMSE Comparison
# Create a data frame for visualization
rmse_data <- data.frame(
  Model = c("Linear Regression", "Linear Regression", "Random Forest", "Random Forest"),
  Density = c("High Density", "Low Density", "High Density", "Low Density"),
  RMSE = c(2.305, 2.655, 0.785, 1.380)
)

# Plot the RMSE values
ggplot(rmse_data, aes(x = Density, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "RMSE Comparison by Model and Population Density",
       x = "Population Density",
       y = "RMSE",
       fill = "Model") +
  theme_minimal()
```

The RMSE for the high population density counties for the Random Forest Model is 0.785, while the RMSE for the low population density counties is 1.380. These numbers are big increases from the Linear Model, as well as from the Random Forest Model that didn't separate high and low population density areas.

## Results & Discussion 

### Exploratory Data Analysis (EDA)

The Exploratory Data Analysis (EDA) revealed key insights into the distribution of air pollution levels (PM2.5 concentrations) and their relationship with various predictors across the U.S. The data consisted of 876 observations and 50 variables, with no missing values. California emerged as a standout state with the highest number of monitors and higher pollution levels compared to other states, demonstrating greater variability in pollution levels within the state.

We observe a positive correlation between the number of monitors in a state and the average pollution levels, suggesting that monitoring intensity may be higher in more polluted areas. Population density showed a slight positive correlation with pollution levels, but the relationship was weak, with most counties having low population density. Variables related to impervious surfaces, road lengths, and emissions were highly correlated within their respective groups, indicating potential redundancy. Interestingly, educational attainment variables exhibited minimal correlation with other predictors, suggesting a limited role in explaining pollution levels.

### Main Question

The primary objective was to predict annual average PM2.5 concentrations across the U.S. using machine learning, specifically a Random Forest model. After training and tuning, the model achieved a final RMSE of 1.72 and an R² of 0.603 on the test set, indicating moderate prediction accuracy of the model. Feature importance analysis identified the "state" variable as the most influential predictor, aligning with the observed differences in pollution levels across states.

Visual comparisons between predicted and observed PM2.5 levels revealed that the Random Forest model effectively captured spatial patterns of air pollution across the U.S, with minimal discrepancies in most regions. However, areas with sparse monitoring data exhibited greater prediction errors, emphasizing the potential benefits of incorporating additional localized data to improve accuracy.

### Extension Question

To explore the relationship between population density and prediction accuracy, we further divided into high- and low-density counties using the median population density as a threshold, which allowed for a targeted analysis of model performance across differing demographic and environmental contexts.

In high-density counties, the Random Forest model achieved an RMSE of 0.785, while in low-density counties, the RMSE increased to 1.38. This difference highlights the model's superior performance in areas with higher population densities, where patterns in pollution and predictor variables may be more consistent. When compared to linear regression models, which yielded RMSE values of 2.31 and 2.67 for high- and low-density counties, respectively, the Random Forest model demonstrated markedly better accuracy across both groups. This result reinforces the robustness of the Random Forest approach, particularly in capturing complex, nonlinear relationships in high-density regions.

## Conclusion

Random Forest model demonstrated reasonable accuracy in predicting annual average PM2.5 concentrations across the U.S., with an RMSE of 1.72 and an R² of 60.3%. The "state" variable emerged as the most critical predictor, with California's elevated pollution levels and variability emphasizing the importance of localized data. The extension analysis showed that population density significantly influences model performance. The Random Forest model performed better in high-density counties, while Linear regression models reflected similar trends with less accuracy. 

However, this study has limitations. The variability in model performance across population density groups suggests that a single model may not capture all regional complexities equally. Areas with sparse monitoring data remain a challenge, as the lack of localized information can lead to greater prediction errors. Additionally, while the Random Forest model demonstrated strong overall performance, further exploration of feature selection and alternative modeling approaches could enhance predictive accuracy, particularly in low-density regions. Future research should focus on integrating additional localized data, such as weather patterns or real-time emissions, to further refine model predictions. Furthermore, the development of density-specific or region-specific models could address the observed disparities in accuracy, ensuring more reliable predictions across diverse environments.

## References

Ellis, Shannon. (2024, November 5). 09-cs02-intro [Lecture]. University of California-San Diego, San Diego, CA, USA. https://cogs137-fa24.github.io/cogs137-fa24/content/lectures/09-cs02-intro.html#still-an-issue

https://www.epa.gov/pm-pollution/particulate-matter-pm-basics
https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2020.00014/full
https://www.opencasestudies.org/ocs-bp-air-pollution/
